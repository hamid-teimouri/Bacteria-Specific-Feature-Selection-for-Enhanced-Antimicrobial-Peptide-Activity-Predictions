{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.5\n",
      "1.4.4\n",
      "3.9.15 (main, Nov 24 2022, 08:29:02) \n",
      "[Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "import sys\n",
    "print(sys.version)\n",
    "import sklearn\n",
    "from scipy.stats import ttest_ind\n",
    "import scipy.stats  as stats\n",
    "#print(sklearn.__version__)\n",
    "from statsmodels.stats.weightstats import ztest as ztest\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns list of all UNIQUE AMPs and their sequences from Angela's dataset\n",
    "def AMP_list(df1,file_name):\n",
    "    import pandas as pd\n",
    "    amp_list=[ ]\n",
    "    amp_seq=[ ]\n",
    "    file_name = open(file_name,\"a\")\n",
    "\n",
    "    for j in range(df1.shape[0]):\n",
    "        element1=df1['peptide I'].loc[j]\n",
    "        element2=df1['peptide II'].loc[j]\n",
    "        seq1=df1['seq I'].loc[j]\n",
    "        seq2=df1['seq II'].loc[j]\n",
    "        s1=seq1.replace(\" \", \"\")\n",
    "        s2=str(seq2).replace(\" \", \"\")\n",
    "        sq1=s1.replace(\"-\", \"\")\n",
    "        sq2=s2.replace(\"-\", \"\")\n",
    "     #print(type(s1),type(s2))\n",
    "\n",
    "        if type(element1) is not float and element1 not in amp_list:\n",
    "        #print(\"peptide I is:\", df1['peptide I'].loc[j],\", And seq is:\" , df1['seq I'].loc[j])\n",
    "        #print(\"peptide II is:\", df1['peptide II'].loc[j],\", And seq is:\" , df1['seq II'].loc[j])\n",
    "            amp_list.append(element1)\n",
    "            amp_seq.append(sq1)\n",
    "            file_name.write('\\n%s' % '>'+element1)\n",
    "            if pd.isnull(seq1) is False :\n",
    "                print(element1,sq1.upper())\n",
    "                file_name.write('\\n%s' % sq1.upper())\n",
    "\n",
    "        if type(element2) is not float and element2 not in amp_list:\n",
    "            amp_list.append(element2)\n",
    "            amp_seq.append(sq2)\n",
    "            file_name.write('\\n%s' % '>'+element2)\n",
    "            if pd.isnull(seq2) is False :\n",
    "                print(element2,sq2.upper())\n",
    "                file_name.write('\\n%s' % sq2.upper())\n",
    "\n",
    "            \n",
    "    file_name.close()\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a dictionary\n",
    "\n",
    "def AMP_pairs_dict(df1):\n",
    "    amp_list=[ ]\n",
    "    fic_list=[ ]\n",
    "    amp_dicts = {}\n",
    "    amp_dicts2 = {}\n",
    "    i=0\n",
    "    for j in range(df1.shape[0]):\n",
    "        element1=df1['peptide I'].loc[j]\n",
    "        element2=df1['peptide II'].loc[j]\n",
    "        element_fic=df1['FICI'].loc[j]\n",
    "        #print(element1, element2,element_fic)\n",
    " \n",
    "        if type(element1) is not float and element1 not in amp_list:\n",
    "            amp_list.append(element1)\n",
    "            if type(element2) is not float and element2 not in amp_list:\n",
    "                i=i+1\n",
    "                amp_list.append(element2)\n",
    "                #print(element1,element2)\n",
    "                amp_dicts[element1] = element2\n",
    "                amp_dicts2[element_fic] = [element1,element2]\n",
    "                #print(i)\n",
    "    return  amp_dicts,amp_dicts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amp_names(input_file):\n",
    "    n_data= []\n",
    "    myfile = open(input_file, 'r')\n",
    "    for line in myfile:\n",
    "        if  line.startswith('>'):\n",
    "            line=line[0:-1]\n",
    "            newstr = line.replace(\">\", \"\")\n",
    "            #print(newstr)\n",
    "            n_data.append(newstr)\n",
    "        \n",
    "    return n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean_distance (df,AMP_A,AMP_B):\n",
    "    D_e=np.linalg.norm(df.loc[AMP_A] - df.loc[AMP_B])\n",
    "    #D_e=np.linalg.norm(AMP_A - AMP_B)\n",
    "\n",
    "\n",
    "    return D_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# this function returns a dataframe containing AMP pairs, metric distance and corresponding FIC\n",
    "def metric_amp(df,my_amps):\n",
    "    name_list2=df.index\n",
    "    list_fic=[]\n",
    "    list_d=[]\n",
    "    for x , y in my_amps[1].items():\n",
    "        if (y[0] in name_list2):\n",
    "            amp_distance=Euclidean_distance (df, y[0], y[1])\n",
    "            #print(x,y[0],y[1],amp_distance)\n",
    "            list_d.append(amp_distance)\n",
    "            list_fic.append(x)\n",
    "        \n",
    "        \n",
    "    return list_fic,list_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import propy\n",
    "from propy import PyPro\n",
    "from propy.PyPro import GetProDes\n",
    "from pathlib import Path  \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_charge(proseq):\n",
    "    \n",
    "    chargeDict = {\"A\":0, \"C\":0, \"D\":-1, \"E\":-1, \"F\":0, \"G\":0, \"H\":1, \"I\":0, \"K\":1, \"L\":0, \"M\":0, \"N\":0, \"P\":0, \"Q\":0, \"R\":1, \"S\":0, \"T\":0, \"V\":0, \"W\":0, \"Y\":0, \"X\":0}\n",
    "    netCharge = sum([chargeDict[x] for x in proseq])\n",
    "    #print(proseq, netCharge)\n",
    "    return netCharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq0='GPDSNHDRGLCRVGNCNPGEYLAKYCFEPVILCKPLSPTPTKT'\n",
    "\n",
    "\n",
    "def standard_aa_fun(seq):\n",
    "    \n",
    "    chargeDict = {\"A\":0, \"C\":0, \"D\":-1, \"E\":-1, \"F\":0, \"G\":0, \"H\":1, \"I\":0, \"K\":1, \"L\":0, \"M\":0, \"N\":0, \"P\":0, \"Q\":0, \"R\":1, \"S\":0, \"T\":0, \"V\":0, \"W\":0, \"Y\":0}\n",
    "    standard_aa=list(chargeDict.keys())\n",
    "    val=1\n",
    "    for element in seq:\n",
    "        if element not in standard_aa:\n",
    "            val=0\n",
    "            #print(element,val)\n",
    "            break\n",
    "        \n",
    "    return val   \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_aa_fun(seq0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def All_Descriptors(input_file):\n",
    "    dfs = []   \n",
    "    myfile = open(input_file, 'r')\n",
    "    i=0\n",
    "    for line in myfile:\n",
    "        if not line.startswith('>'):\n",
    "            line=line[0:-1]\n",
    "            arg=standard_aa_fun(line)\n",
    "            if arg==1:\n",
    "                i=i+1\n",
    "                #print(line,arg,i)\n",
    "                Des=GetProDes(line)\n",
    "                alldes = Des.GetALL()\n",
    "            df = pd.DataFrame([alldes])\n",
    "            dfs.append(df)\n",
    "            ddf=pd.concat(dfs) \n",
    "                \n",
    "                #filepath = Path('all_descriptors.csv')  \n",
    "                #filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "                #ddf.to_csv(filepath)\n",
    "                \n",
    "            #elif arg==0:\n",
    "            #    break\n",
    "\n",
    "    myfile.close()     \n",
    "    return ddf.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_charge_calculator(input_file):\n",
    "    c_data= []\n",
    "    myfile = open(input_file, 'r')\n",
    "    for line in myfile:\n",
    "        #print(line)\n",
    "        if not line.startswith('>'):\n",
    "            line=line[0:-1]\n",
    "            net_c=total_charge(line)\n",
    "            c_data.append(net_c)\n",
    "            \n",
    "    return pd.DataFrame(c_data,columns=[\"net_charge\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Descriptors_complete(neg,csv_file):\n",
    "    nc_df=net_charge_calculator(neg)\n",
    "\n",
    "    all_df=All_Descriptors(neg)\n",
    "    df_names=pd.DataFrame(amp_names(neg),columns=[\"AMP_Name\"])\n",
    "    result1 = pd.merge(nc_df,df_names, left_index=True, right_index=True)\n",
    "    result = pd.merge(result1,all_df, left_index=True, right_index=True)\n",
    "    #filepath = Path(csv_file)  \n",
    "    #filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "    #result.to_csv(filepath)  \n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AMP_list2(df1,file_name):\n",
    "    import pandas as pd\n",
    "    amp_list=[ ]\n",
    "    amp_seq=[ ]\n",
    "    amp_dict1 = {}\n",
    "    amp_dict2 = {}\n",
    "\n",
    "    \n",
    "    file_name = open(file_name,'w')\n",
    "\n",
    "    for j in range(df1.shape[0]):\n",
    "        element1=df1['peptide'].loc[j]\n",
    "        seq1=df1['sequence'].loc[j]\n",
    "        s1=seq1.replace(\" \", \"\")\n",
    "        sq1=s1.replace(\"-\", \"\")\n",
    "        #print(j,element1)\n",
    "        if type(element1) is not float and element1 not in amp_list:\n",
    "            amp_list.append(element1)\n",
    "            amp_dict1[element1]=df1['MIC'].loc[j]\n",
    "            amp_dict2[element1]=df1['MIC_0'].loc[j]\n",
    "\n",
    "            amp_seq.append(sq1)\n",
    "            file_name.write('>' + element1 )\n",
    "            #file_name.write('\\n%s' % '>' + element1)\n",
    "            if pd.isnull(seq1) is False :\n",
    "                #print(element1,sq1.upper())\n",
    "                file_name.write('\\n%s' % sq1.upper() + '\\n' )\n",
    "\n",
    "       \n",
    "            \n",
    "    file_name.close()\n",
    "        \n",
    "    return amp_dict1,amp_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the original dataframe a returns a ready to use dataset\n",
    "def final_df(df,textfile,csvfile):\n",
    "\n",
    "    dict1,dict2=AMP_list2(df,textfile)\n",
    "    #print(dict1)\n",
    "    df1=Descriptors_complete(textfile, csvfile)\n",
    "    filepath = Path(csvfile)  \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)      \n",
    "    #df1=pd.read_csv(csvfile, )\n",
    "    #df2=df1.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    df1['MIC']=list(dict1.values())\n",
    "    df1['MIC_0']=list(dict2.values())\n",
    "    \n",
    "    df1.to_csv(filepath)  \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_df(df):\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    df0=df.drop(columns=['AMP_Name'])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_normalized = scaler.fit_transform(df0)\n",
    "    scaled_features_df = pd.DataFrame(df_normalized, index=df0.index, columns=df0.columns)\n",
    "\n",
    "    return scaled_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need function for multiple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/ Algorithms\n",
    "\n",
    "def models(X_train,Y_train):\n",
    "        #logistic regression\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        log=LogisticRegression(random_state=0)\n",
    "        log.fit(X_train,Y_train)\n",
    "        \n",
    "        \n",
    "        #Decision Tree\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        tree=DecisionTreeClassifier(random_state=0,criterion=\"entropy\")\n",
    "        tree.fit(X_train,Y_train)\n",
    "        \n",
    "        #Random Forest\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        forest=RandomForestClassifier(random_state=0,criterion=\"entropy\",n_estimators=10)\n",
    "        forest.fit(X_train,Y_train)\n",
    "    \n",
    "        \n",
    "        print('[0]logistic regression accuracy:',log.score(X_train,Y_train))\n",
    "        print('[1]Decision tree accuracy:',tree.score(X_train,Y_train))\n",
    "        print('[2]Random forest accuracy:',forest.score(X_train,Y_train))\n",
    "\n",
    "        return log,tree,forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the original dataframe a returns a ready to use dataset\n",
    "def final_df2(df,textfile,csvfile):\n",
    "\n",
    "    dict1,dict2=AMP_list0(df,textfile)\n",
    "    #print(dict1)\n",
    "    df1=Descriptors_complete(textfile, csvfile)\n",
    "    filepath = Path(csvfile)  \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)      \n",
    "    #df1=pd.read_csv(csvfile, )\n",
    "    #df2=df1.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    df1['MIC']=list(dict1.values())\n",
    "    df1['MIC_0']=list(dict2.values())\n",
    "    \n",
    "    df1.to_csv(filepath)  \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AMP_list0(df1,file_name):\n",
    "    import pandas as pd\n",
    "    amp_list=[ ]\n",
    "    amp_seq=[ ]\n",
    "    amp_dict1 = {}\n",
    "    amp_dict2 = {}\n",
    "\n",
    "    \n",
    "    file_name = open(file_name,'w')\n",
    "\n",
    "    for j in range(df1.shape[0]):\n",
    "        #print(j)\n",
    "        element1=df1['peptide'].loc[j]\n",
    "        seq1=df1['sequence'].loc[j]\n",
    "        s1=seq1.replace(\" \", \"\")\n",
    "        sq1=s1.replace(\"-\", \"\")\n",
    "        #print(j,element1)\n",
    "        if type(element1) is not float and element1 not in amp_list:\n",
    "            amp_list.append(element1)\n",
    "            amp_dict1[element1]=df1['MIC'].loc[j]\n",
    "            amp_dict2[element1]=df1['MIC_0'].loc[j]\n",
    "\n",
    "            amp_seq.append(sq1)\n",
    "            file_name.write('>' + str(element1) )\n",
    "            #file_name.write('\\n%s' % '>' + element1)\n",
    "            if pd.isnull(seq1) is False :\n",
    "                #print(element1,sq1.upper())\n",
    "                file_name.write('\\n%s' % sq1.upper() + '\\n' )\n",
    "\n",
    "    file_name.close()\n",
    "        \n",
    "    return amp_dict1,amp_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "def cross_val_split(X,y,ns):\n",
    "    i = 0\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    Xtrain_cv = list(range(ns))\n",
    "    Xtest_cv = list(range(ns))\n",
    "    Ytrain_cv = list(range(ns))\n",
    "    Ytest_cv = list(range(ns))\n",
    "    cv = StratifiedShuffleSplit(n_splits=ns, test_size=0.2, random_state=999)\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        Xtrain_cv[i] = X[train_index]\n",
    "        Xtest_cv[i] = X[test_index]\n",
    "        Ytrain_cv[i] = y[train_index]\n",
    "        Ytest_cv[i] = y[test_index]\n",
    "        i+=1\n",
    "    return Xtrain_cv, Xtest_cv, Ytrain_cv, Ytest_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_SVM_grid_search(X,y,ns):\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 20)\n",
    "    model = LinearSVC()\n",
    "    C_range = np.logspace(-1,1,3)\n",
    "    param_grid = {\"C\": C_range}\n",
    "    scoring = ['accuracy']\n",
    "    kfold = StratifiedShuffleSplit(n_splits=ns, test_size=0.2, random_state=999)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid=param_grid,\n",
    "                           refit='accuracy',\n",
    "                           n_jobs=-1,\n",
    "                           cv=kfold,\n",
    "                           verbose=0)\n",
    "    grid_result = grid_search.fit(X_train,Y_train)\n",
    "    CC=grid_result.best_params_['C']\n",
    "    return CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_feature_selection(Xtrain_cv,Ytrain_cv,CC,ns,klist):\n",
    "    from sklearn.svm import LinearSVC\n",
    "    import numpy as np\n",
    "    listlocs = list(range(ns))\n",
    "    results = list(range(ns))\n",
    "    kfeatures = list(range(ns))\n",
    "    kweights = list(range(ns))\n",
    "\n",
    "    bagging_dict = {}\n",
    "    for i in range(ns): # loop over cross validation sets\n",
    "        model = LinearSVC()\n",
    "        params = {\"penalty\":\"l1\", \"C\":CC, \"loss\":\"squared_hinge\", \"dual\":False, \"max_iter\": 1000000}\n",
    "        model.set_params(**params)\n",
    "    \n",
    "        X_train = Xtrain_cv[i]\n",
    "        Y_train = Ytrain_cv[i]\n",
    "# coef_ is a list of weights same as w\n",
    "        model.fit(X_train, Y_train)\n",
    "        T = model.coef_[0]\n",
    "        TT = list(T)\n",
    "        locs = np.where(abs(T) > 0)\n",
    "        listlocs[i] = np.asarray(locs)\n",
    "\n",
    "        if i == 0:\n",
    "            flistlocs = listlocs[i]\n",
    "            flistlocs=flistlocs[0]\n",
    "        else:\n",
    "            flistlocs = np.intersect1d(flistlocs,listlocs[i])\n",
    "        \n",
    "\n",
    "        for f in flistlocs:\n",
    "            f_feature = klist[f]\n",
    "            f_weight = T[f]\n",
    "        \n",
    "            if i == 0:\n",
    "                bagging_dict[f_feature] = []\n",
    "                bagging_dict[f_feature].append(f_weight)\n",
    "            else:\n",
    "                bagging_dict[f_feature].append(f_weight)\n",
    "\n",
    "        kfeatures[i] = klist[listlocs[i]]\n",
    "        kweights[i] = T[listlocs[i]]\n",
    "        \n",
    "    kfeatures=klist[flistlocs]\n",
    "    featuredict={}\n",
    "    for k,v in bagging_dict.items():\n",
    "        if k in kfeatures:\n",
    "            print(k, sum(v)/len(v))\n",
    "            featuredict[k]=sum(v)/len(v)\n",
    "    return featuredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_feature_plot(featuredict,fig_title,fonts):\n",
    "    featuredict = sorted(featuredict.items(), key=lambda x:x[1])\n",
    "    feature_name=[]\n",
    "    feature_coef=[]\n",
    "\n",
    "    for items in featuredict:\n",
    "        #print(items[0], items[1])\n",
    "        feature_name.append(items[0])\n",
    "        feature_coef.append(items[1])\n",
    "    xx=feature_name\n",
    "    yy=feature_coef\n",
    "    # Plot of feature importance\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    plt.figure(figsize=(7, 7))\n",
    "    #sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.barplot(yy, xx)\n",
    "    plt.xlabel(\"Feature Importance\",  fontsize=fonts)\n",
    "    \n",
    "    plt.savefig(fig_title, bbox_inches = 'tight',\n",
    "        pad_inches = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_grid_search(X,y):\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from numpy import arange\n",
    "    # define model\n",
    "    model = Lasso()\n",
    "    # define model evaluation method\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define grid\n",
    "    grid = dict()\n",
    "    grid['alpha'] = arange(0, 1, 0.01)\n",
    "    # define search\n",
    "    search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    # perform the search\n",
    "    results = search.fit(X, y)\n",
    "    AA = results.best_params_['alpha']\n",
    "    print(AA)\n",
    "    return AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_feature_selection(Xtrain_cv,Ytrain_cv,CC,ns,klist):\n",
    "    from sklearn.linear_model import Lasso\n",
    "    import numpy as np\n",
    "    listlocs = list(range(ns))\n",
    "    results = list(range(ns))\n",
    "    kfeatures = list(range(ns))\n",
    "    kweights = list(range(ns))\n",
    "\n",
    "    bagging_dict = {}\n",
    "    model = Lasso(alpha = AA)\n",
    "    for i in range(ns):\n",
    "    \n",
    "        X_train = Xtrain_cv[i]\n",
    "        Y_train = Ytrain_cv[i]\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        T = model.coef_[0]\n",
    "        TT = list(T)\n",
    "        locs = np.where(abs(T) > 0)\n",
    "        listlocs[i] = np.asarray(locs)\n",
    "\n",
    "        if i == 0:\n",
    "            flistlocs = listlocs[i]\n",
    "            flistlocs=flistlocs[0]\n",
    "        else:\n",
    "            flistlocs = np.intersect1d(flistlocs,listlocs[i])\n",
    "        \n",
    "\n",
    "        for f in flistlocs:\n",
    "            f_feature = klist[f]\n",
    "            f_weight = T[f]\n",
    "        \n",
    "            if i == 0:\n",
    "                bagging_dict[f_feature] = []\n",
    "                bagging_dict[f_feature].append(f_weight)\n",
    "            else:\n",
    "                bagging_dict[f_feature].append(f_weight)\n",
    "\n",
    "        kfeatures[i] = klist[listlocs[i]]\n",
    "        kweights[i] = T[listlocs[i]]\n",
    "        \n",
    "    kfeatures=klist[flistlocs]\n",
    "    featuredict={}\n",
    "    for k,v in bagging_dict.items():\n",
    "        if k in kfeatures:\n",
    "            print(k, sum(v)/len(v))\n",
    "            featuredict[k]=sum(v)/len(v)\n",
    "    return featuredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(X_train,Y_train):\n",
    "        #logistic regression\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        log1=LogisticRegression(random_state=0)\n",
    "        \n",
    "        log1.fit(X_train,Y_train)\n",
    "        \n",
    "        from sklearn.svm import SVC\n",
    "        svc_model = SVC()\n",
    "        svc_model.fit(X_train, Y_train)\n",
    "\n",
    "        log = log1.score(X_train,Y_train)\n",
    "        svc = svc_model.score(X_train, Y_train)\n",
    "\n",
    "        y_pred_log=log1.predict(X_train)\n",
    "        y_pred_svm=svc_model.predict(X_train)\n",
    "\n",
    "        rec_log=recall_score(Y_train, y_pred_log, average='macro')\n",
    "        rec_svm=recall_score(Y_train, y_pred_svm, average='macro')\n",
    "        mat_log= matthews_corrcoef(Y_train, y_pred_log)\n",
    "        mat_svm= matthews_corrcoef(Y_train, y_pred_svm)\n",
    "\n",
    "        #print('[0]logistic regression accuracy:',log)\n",
    "        #print('[1]SVM accuracy:',svc)\n",
    "        #print('[2]logistic regression recall:',rec_log)\n",
    "        #print('[3]SVM regression recall:',rec_svm)\n",
    "\n",
    "        #recall_score(y_true, y_pred, average='macro')\n",
    "        return log,svc,rec_log,rec_svm,mat_log,mat_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_crossval(Xtrain_cv,Ytrain_cv,ns):\n",
    "    log_list = []\n",
    "    svc_list = []\n",
    "    rec_log_list = []\n",
    "    rec_svc_list = []\n",
    "    mat_log_list = []\n",
    "    mat_svc_list = []\n",
    "    \n",
    "    \n",
    "    for i in range(ns):\n",
    "        log,svc,r_log,r_svm,m_log,m_svm = models(Xtrain_cv[i],Ytrain_cv[i])\n",
    "        log_list.append(log)\n",
    "        svc_list.append(svc)\n",
    "        rec_log_list.append(r_log)\n",
    "        rec_svc_list.append(r_svm)\n",
    "        mat_log_list.append(m_log)\n",
    "        mat_svc_list.append(m_svm)\n",
    "        \n",
    "    log_array = np.array(log_list)\n",
    "    svc_array = np.array(svc_list)\n",
    "    r_log_array = np.array(rec_log_list)\n",
    "    r_svc_array = np.array(rec_svc_list)\n",
    "    m_log_array = np.array(mat_log_list)\n",
    "    m_svc_array = np.array(mat_svc_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    log_mean = np.mean(log_array)\n",
    "    svc_mean = np.mean(svc_array)\n",
    "    r_log_mean = np.mean(r_log_array)\n",
    "    r_svc_mean = np.mean(r_svc_array)\n",
    "    m_log_mean = np.mean(m_log_array)\n",
    "    m_svc_mean = np.mean(m_svc_array) \n",
    "        \n",
    "        \n",
    "    print('average logistic regression accuracy:'+str(log_mean))\n",
    "    print('average SVM accuracy:'+str(svc_mean))\n",
    "    print('average logistic regression recall:'+str(r_log_mean))\n",
    "    print('average SVM recall:'+str(r_svc_mean))\n",
    "    print('average logistic regression matthews_corrcoef:'+str(m_log_mean))\n",
    "    print('average SVM matthews_corrcoef:'+str(m_svc_mean))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_function(X,y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 20)\n",
    "    from sklearn.svm import SVC\n",
    "    svc_model = SVC()\n",
    "    svc_model.fit(X_train, y_train)\n",
    "    y_predict = svc_model.predict(X_test)\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    cm = np.array(confusion_matrix(y_test, y_predict, labels=[1,0]))\n",
    "    confusion = pd.DataFrame(cm, index=['AMP', 'non_AMP'],\n",
    "                         columns=['predicted_AMP','predicted_nonAMP'])\n",
    "    class_report = classification_report(y_test,y_predict)\n",
    "    print(class_report)\n",
    "    return svc_model,confusion,class_report,y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each bacterium we need X, y, and klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary for each bacterium\n",
    "def bacterium_load(bacterium_file_name,data_dict,bacterium_name):\n",
    "    ddf=pd.read_csv(bacterium_file_name)\n",
    "    df2=ddf.drop(columns=['Unnamed: 0'])\n",
    "    df_norm=normalized_df(df2)\n",
    "    y = df2['MIC_0']\n",
    "    dff2 = df2.drop(columns=['AMP_Name','MIC','MIC_0'])\n",
    "    klist = np.array(dff2.columns)\n",
    "    #first z score\n",
    "    from scipy import stats\n",
    "\n",
    "    X = stats.zscore(dff2, axis = 1, ddof = 1, nan_policy = 'raise')\n",
    "    data_dict[bacterium_name] = list(range(3))\n",
    "    # also need: featuredict for \n",
    "    data_dict[bacterium_name][0] = X\n",
    "    data_dict[bacterium_name][1] = y\n",
    "    data_dict[bacterium_name][2] = klist\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redundant_filter(all_des):\n",
    "    redundant_cols = [ ]\n",
    "\n",
    "    for column1 in all_des:\n",
    "        for column2 in all_des:\n",
    "            if column1!=column2:\n",
    "                Cor_sp=stats.spearmanr(all_des[column1], all_des[column2])\n",
    "                if Cor_sp[0]>0.95:\n",
    "                    #print(column1,column2, Cor_sp[0] , Cor_sp[1] )\n",
    "                    if column1 not in redundant_cols:\n",
    "                        redundant_cols.append(column1)\n",
    "                        #print(redundant_cols)\n",
    "    return redundant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we write a function that returns common elemnts of the two lists\n",
    "\n",
    "def common_member(a, b):\n",
    "    \n",
    "    a_set = set(a)\n",
    "    b_set = set(b)\n",
    " \n",
    "    if (a_set & b_set):\n",
    "        print(a_set & b_set)\n",
    "    else:\n",
    "        print(\"No common elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plotter(ddf2,feature_name,num):\n",
    "\n",
    "    from statsmodels.stats.weightstats import ztest as ztest\n",
    "    plt.style.use('classic')\n",
    "\n",
    "    plt.figure()\n",
    "    nonAMP_feature=ddf2[ddf2['MIC_0']==0][feature_name]\n",
    "    AMP_feature=ddf2[ddf2['MIC_0']==1][feature_name]\n",
    "    x1 = list(nonAMP_feature)\n",
    "\n",
    "    x2 = list(AMP_feature)\n",
    "    plt.hist(x2, density=True, bins=num, color='green', label='AMP')  # density=False would make counts\n",
    "    plt.hist(x1, density=True, bins=num, color='red', alpha = 0.5, label='nonAMP')  # density=False would make counts\n",
    "    \n",
    "    plt.ylabel('Frequency', fontsize = 20)\n",
    "    plt.xlabel(feature_name, fontsize = 20);\n",
    "    plt.legend(loc='upper right', fontsize =15)\n",
    "    plt.title((ztest(x1, x2, value=0) ))\n",
    "    plt.savefig('hist_' + feature_name + '.pdf')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_filter(ddf,seq_len):\n",
    "\n",
    "    ll_list= []\n",
    "    for i in range(len(ddf['sequence'])):\n",
    "    #print(i)\n",
    "        ll=len(ddf['sequence'].iloc[i])\n",
    "    #print(ll)\n",
    "        ll_list.append(ll)\n",
    "\n",
    "    df['seq_length']=ll_list\n",
    "    \n",
    "    return df[df['seq_length']>seq_len]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redundant_filter(all_des):\n",
    "    redundant_cols = [ ]\n",
    "\n",
    "    for column1 in all_des:\n",
    "        for column2 in all_des:\n",
    "            if column1!=column2:\n",
    "                Cor_sp=stats.spearmanr(all_des[column1], all_des[column2])\n",
    "                if Cor_sp[0]>0.95:\n",
    "                    #print(column1,column2, Cor_sp[0] , Cor_sp[1] )\n",
    "                    if column1 not in redundant_cols:\n",
    "                        redundant_cols.append(column1)\n",
    "                        #print(redundant_cols)\n",
    "    return redundant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ztest(df2):\n",
    "    import numpy as np\n",
    "    import pingouin as pg\n",
    "    feature_z = {}\n",
    "    i=0\n",
    "    for columns in df2:\n",
    "        nonAMP_feature=df2[df2['MIC_0']==0][columns]\n",
    "        AMP_feature=df2[df2['MIC_0']==1][columns]\n",
    "        x1 = list(nonAMP_feature)\n",
    "        x2 = list(AMP_feature)\n",
    "        #z_val=ztest(x1, x2, value=0) \n",
    "        z_val=pg.mwu(x1, x2, alternative='two-sided')\n",
    "        #if np.abs(z_val[0])>critical_z:\n",
    "        #    i+=1\n",
    "        print(columns, np.abs(z_val[0]),z_val[1],i)\n",
    "        #    feature_z[columns] = np.abs(z_val[0])\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mann_whitney_test(df1):\n",
    "\n",
    "#z_dict=feature_ztest(df1)\n",
    "    import numpy as np\n",
    "    import pingouin as pg\n",
    "    feature_u = []\n",
    "    \n",
    "    for columns in df1.columns:\n",
    "        nonAMP_feature=df1[df1['MIC_0']==0][columns]\n",
    "        AMP_feature=df1[df1['MIC_0']==1][columns]\n",
    "        x1 = list(nonAMP_feature)\n",
    "        x2 = list(AMP_feature)\n",
    "        #z_val=ztest(x1, x2, value=0) \n",
    "        z_val=pg.mwu(x1, x2, alternative='two-sided')\n",
    "        #tmp1 = np.float(z_val['RBC'])\n",
    "        #tmp2 = np.float(z_val['CLES'])\n",
    "        #print(columns)\n",
    "        #print(columns,z_val)\n",
    "        #print(columns,np.float(z_val['p-val']),np.float(z_val['U-val']),np.float(z_val['RBC']),np.float(z_val['CLES']))\n",
    "\n",
    "        if np.float(z_val['RBC'])<-0.3  and np.float(z_val['CLES'])>0.5:\n",
    "            print(columns,np.float(z_val['p-val']),np.float(z_val['U-val']),np.float(z_val['RBC']),np.float(z_val['CLES']))\n",
    "            feature_u.append(columns)\n",
    "        \n",
    "        \n",
    "    return feature_u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
